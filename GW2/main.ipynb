{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peewee'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myfinance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas_ta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mta\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hmtme\\git\\wqu-fd-gw\\.venv\\Lib\\site-packages\\yfinance\\__init__.py:23\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01msearch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Search\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ticker\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtickers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tickers\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hmtme\\git\\wqu-fd-gw\\.venv\\Lib\\site-packages\\yfinance\\search.py:26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconst\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _BASE_URL_\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YfData\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSearch\u001b[39;00m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, max_results=\u001b[32m8\u001b[39m, news_count=\u001b[32m8\u001b[39m, lists_count=\u001b[32m8\u001b[39m, include_cb=\u001b[38;5;28;01mTrue\u001b[39;00m, include_nav_links=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     31\u001b[39m                  include_research=\u001b[38;5;28;01mFalse\u001b[39;00m, include_cultural_assets=\u001b[38;5;28;01mFalse\u001b[39;00m, enable_fuzzy_query=\u001b[38;5;28;01mFalse\u001b[39;00m, recommended=\u001b[32m8\u001b[39m,\n\u001b[32m     32\u001b[39m                  session=\u001b[38;5;28;01mNone\u001b[39;00m, proxy=\u001b[38;5;28;01mNone\u001b[39;00m, timeout=\u001b[32m30\u001b[39m, raise_errors=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hmtme\\git\\wqu-fd-gw\\.venv\\Lib\\site-packages\\yfinance\\data.py:11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfrozendict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frozendict\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, cache\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconst\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m USER_AGENTS\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hmtme\\git\\wqu-fd-gw\\.venv\\Lib\\site-packages\\yfinance\\cache.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeewee\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_peewee\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Lock\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_os\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'peewee'"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppress specific warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message=\"Liblinear failed to converge*\") # Added for potential solver issues\n",
    "\n",
    "# @title Data Download and Feature Engineering\n",
    "# --- 1. Select ETF and Download Data ---\n",
    "etf_ticker = \"EWZ\"\n",
    "# Use dates roughly corresponding to the cleaned data in the paper\n",
    "# Original paper data: 2009-12-12 to 2020-01-01 [cite: 43]\n",
    "# Cleaned data starts later due to indicator calculation [cite: 89]\n",
    "# Let's download a slightly wider range initially to allow for indicator calculation\n",
    "start_date = \"2009-01-01\"\n",
    "end_date = \"2020-01-02\" # End date is exclusive in yfinance usually\n",
    "\n",
    "data = yf.download(etf_ticker, start=start_date, end=end_date)\n",
    "\n",
    "print(f\"Downloaded {len(data)} data points for {etf_ticker}\")\n",
    "\n",
    "# --- 2. Calculate Target Variable (Gamma) ---\n",
    "# Target: 1 if Open(t) > Open(t-1), 0 otherwise\n",
    "data['Gamma'] = (data['Open'].diff() > 0).astype(int)\n",
    "# Shift target to align with previous day's features (predict t based on t-1 info)\n",
    "data['Gamma'] = data['Gamma'].shift(-1)\n",
    "\n",
    "# --- 3. Calculate Technical Indicators (Simplified Set) ---\n",
    "# Using pandas_ta library, calculate a few example indicators\n",
    "# This is a major simplification from the 216 used in the paper [cite: 77]\n",
    "data.ta.sma(length=10, append=True) # Simple Moving Average\n",
    "data.ta.rsi(length=14, append=True) # Relative Strength Index\n",
    "data.ta.bbands(length=20, append=True) # Bollinger Bands (adds 3 columns)\n",
    "data.ta.macd(append=True) # MACD (adds 3 columns)\n",
    "data.ta.atr(append=True) # Average True Range\n",
    "\n",
    "# Keep original OHLCV as features too\n",
    "base_features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "# Dynamically get added ta features (excluding potential NaNs like BB_RP)\n",
    "ta_features = [col for col in data.columns if col.startswith(('SMA', 'RSI', 'BBP', 'BBB', 'BBM', 'MACD', 'MACDh', 'MACDs', 'ATRr'))]\n",
    "\n",
    "all_feature_columns = base_features + ta_features\n",
    "\n",
    "# --- Data Cleaning and Preparation ---\n",
    "# Remove rows with NaN values created by indicators or target shift\n",
    "data_clean = data.dropna()\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data_clean[all_feature_columns].copy()\n",
    "y = data_clean['Gamma'].copy()\n",
    "\n",
    "# Scale features using Min-Max Scaler [cite: 83]\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=all_feature_columns, index=X.index)\n",
    "\n",
    "print(f\"Data prepared with {len(X_scaled)} samples and {len(all_feature_columns)} features.\")\n",
    "print(\"Features:\", all_feature_columns)\n",
    "\n",
    "\n",
    "# @title Feature Selection (Pearson Correlation)\n",
    "# --- 4. Feature Selection using Pearson's Correlation ---\n",
    "# Calculate correlation between each feature and the target variable 'Gamma' [cite: 118]\n",
    "correlations = X_scaled.corrwith(y).abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Correlation with Target (Absolute Values) ---\")\n",
    "print(correlations)\n",
    "\n",
    "# Get the ranked list of features\n",
    "ranked_features = correlations.index.tolist()\n",
    "\n",
    "\n",
    "# @title Cross-Validation and Model Evaluation\n",
    "# --- 5 & 6. Cross-Validation and MLP Training ---\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42) # Use shuffle for robustness\n",
    "\n",
    "# Define feature subset sizes to test (e.g., top 5, 10, 15, all)\n",
    "# Adjust based on the number of available features\n",
    "max_features = len(ranked_features)\n",
    "# Ensure subset sizes don't exceed max_features\n",
    "subset_sizes = sorted(list(set([min(k, max_features) for k in [5, 10, 15, 20, 25, max_features]])))\n",
    "\n",
    "\n",
    "results = {} # To store accuracy for each subset size\n",
    "\n",
    "print(f\"\\n--- Running {n_splits}-Fold Cross-Validation ---\")\n",
    "\n",
    "for k in subset_sizes:\n",
    "    selected_features = ranked_features[:k]\n",
    "    print(f\"Evaluating with Top {k} features: {selected_features}\")\n",
    "\n",
    "    X_subset = X_scaled[selected_features]\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X_subset, y)):\n",
    "        X_train, X_test = X_subset.iloc[train_index], X_subset.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Define MLP Classifier (parameters roughly based on paper's description)\n",
    "        # Note: Hidden layer size calculation depends on number of features 'k' [cite: 106]\n",
    "        # Paper used logistic activation, lbfgs solver, adaptive learning rate [cite: 95]\n",
    "        # These params can be sensitive and may need tuning. LBFGS is better for smaller datasets.\n",
    "        hidden_layer_size = int((k + len(np.unique(y))) / 2)\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(max(1, hidden_layer_size),), # Ensure at least 1 neuron\n",
    "                            activation='logistic',\n",
    "                            solver='lbfgs', # Good for smaller datasets\n",
    "                            max_iter=1000, # Increased iterations\n",
    "                            learning_rate='adaptive',\n",
    "                            learning_rate_init=0.03, # From paper [cite: 95]\n",
    "                            momentum=0.9, # Default, paper used 0.2 [cite: 95] - keeping default for stability\n",
    "                            random_state=fold, # Use fold number for reproducibility within CV\n",
    "                            early_stopping=False # Paper ran tests with and without [cite: 175]\n",
    "                           )\n",
    "\n",
    "        # Train the model\n",
    "        mlp.fit(X_train, y_train)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    # Calculate average accuracy for this feature subset size\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    results[k] = avg_accuracy\n",
    "    print(f\"Average Accuracy for Top {k} features: {avg_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# @title Results Table and Graph\n",
    "# --- 7. Reproduce Table (Accuracy vs. Feature Count) ---\n",
    "print(\"\\n--- Results Summary (Accuracy vs. No. Top Features by Correlation) ---\")\n",
    "results_df = pd.DataFrame(list(results.items()), columns=['Num_Features', 'Avg_Accuracy'])\n",
    "results_df['Avg_Accuracy'] = results_df['Avg_Accuracy'] * 100 # Display as percentage\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# --- 8. Reproduce Graph (Accuracy Gain Concept) ---\n",
    "baseline_accuracy = results[max_features] # Accuracy using all calculated features\n",
    "results_df['Accuracy_Gain (%)'] = results_df['Avg_Accuracy'] - (baseline_accuracy * 100)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Num_Features'], results_df['Avg_Accuracy'], marker='o', linestyle='-')\n",
    "plt.title(f'MLP Accuracy vs. Number of Top Features (EWZ, Selected by Correlation)')\n",
    "plt.xlabel('Number of Top Features Used')\n",
    "plt.ylabel('Average Cross-Validated Accuracy (%)')\n",
    "plt.grid(True)\n",
    "plt.xticks(results_df['Num_Features']) # Ensure ticks are at the tested feature counts\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy gain (similar concept to Figure 5 [cite: 180])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Num_Features'], results_df['Accuracy_Gain (%)'], marker='o', linestyle='-', color='g')\n",
    "plt.title(f'Accuracy Gain vs. Baseline (EWZ, Selected by Correlation)')\n",
    "plt.xlabel('Number of Top Features Used')\n",
    "plt.ylabel('Accuracy Gain vs. All Features (%)')\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='grey', linestyle='--', linewidth=0.8) # Add horizontal line at 0 gain\n",
    "plt.xticks(results_df['Num_Features'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
